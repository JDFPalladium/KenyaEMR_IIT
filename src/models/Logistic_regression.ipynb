{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d6fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import boto3\n",
    "import tempfile\n",
    "from settings import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce2b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID'] = settings.AWS_ACCESS_KEY_ID\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = settings.AWS_SECRET_ACCESS_KEY\n",
    "os.environ['AWS_DEFAULT_REGION'] = settings.AWS_DEFAULT_REGION\n",
    "\n",
    "# Define S3 info\n",
    "bucket_name = 'kehmisjan2025'\n",
    "file_key = 'targets_apr23.rds'\n",
    "\n",
    "# Initialize boto3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Download to a temporary file\n",
    "with tempfile.NamedTemporaryFile(suffix=\".rds\") as tmp_file:\n",
    "    s3.download_fileobj(bucket_name, file_key, tmp_file)\n",
    "    tmp_file.seek(0)  # go back to beginning\n",
    "    result = pyreadr.read_r(tmp_file.name)  # returns a dictionary\n",
    "\n",
    "# Extract the data frame\n",
    "iit_data = next(iter(result.values()))  # assumes only one object inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968bf69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(iit_data.dtypes) \n",
    "# Ensure the 'NAD' column is converted to datetime\n",
    "iit_data['NAD'] = pd.to_datetime(iit_data['NAD'], format='%Y-%m-%d')\n",
    "# iit_data['VisitDate'] = pd.to_datetime(iit_data['VisitDate'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65190bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the last quarter of the year\n",
    "# Define the date range to exclude\n",
    "start_exclude = pd.Timestamp('2024-10-02')\n",
    "end_exclude = pd.Timestamp('2024-12-31')\n",
    "\n",
    "# Filter out records from Sept through Dec 2024\n",
    "iit_data = iit_data[~((iit_data['NAD'] >= start_exclude) & (iit_data['NAD'] <= end_exclude))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "599398f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10080/1627046203.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iit_data['is_friday'] = iit_data['Day'].apply(lambda x: 1 if x == \"Fri\" else 0)\n"
     ]
    }
   ],
   "source": [
    "# iit_data['is_december'] = iit_data['Month'].apply(lambda x: 1 if x == \"December\" else 0)\n",
    "iit_data['is_friday'] = iit_data['Day'].apply(lambda x: 1 if x == \"Fri\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "733e0c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10080/2117124041.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iit_data['BMI'] = np.where(iit_data['BMI'].isna() & (iit_data['BMI_Missing'] == 0), 'NR', iit_data['BMI'])\n",
      "/tmp/ipykernel_10080/2117124041.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iit_data['ARTAdherence'] = np.where(iit_data['ARTAdherence'].isna() & (iit_data['Adherence_Missing'] == 0), 'NR', iit_data['ARTAdherence'])\n",
      "/tmp/ipykernel_10080/2117124041.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iit_data['Pregnant'] = np.where(iit_data['Pregnant'].isna() & (iit_data['Pregnant_Missing'] == 0), 'NR', iit_data['Pregnant'])\n",
      "/tmp/ipykernel_10080/2117124041.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iit_data['Breastfeeding'] = np.where(iit_data['Breastfeeding'].isna() & (iit_data['Breastfeeding_Missing'] == 0), 'NR', iit_data['Breastfeeding'])\n",
      "/tmp/ipykernel_10080/2117124041.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iit_data['WHOStage'] = np.where(iit_data['WHOStage'].isna() & (iit_data['WHO_Missing'] == 0), 'NR', iit_data['WHOStage'])\n"
     ]
    }
   ],
   "source": [
    "iit_data['BMI'] = np.where(iit_data['BMI'].isna() & (iit_data['BMI_Missing'] == 0), 'NR', iit_data['BMI'])\n",
    "iit_data['ARTAdherence'] = np.where(iit_data['ARTAdherence'].isna() & (iit_data['Adherence_Missing'] == 0), 'NR', iit_data['ARTAdherence'])\n",
    "iit_data['Pregnant'] = np.where(iit_data['Pregnant'].isna() & (iit_data['Pregnant_Missing'] == 0), 'NR', iit_data['Pregnant'])\n",
    "iit_data['Breastfeeding'] = np.where(iit_data['Breastfeeding'].isna() & (iit_data['Breastfeeding_Missing'] == 0), 'NR', iit_data['Breastfeeding'])\n",
    "iit_data['WHOStage'] = np.where(iit_data['WHOStage'].isna() & (iit_data['WHO_Missing'] == 0), 'NR', iit_data['WHOStage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf072d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-04 2024-09-30\n",
      "2022-01-01 00:00:00 2024-10-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(iit_data['VisitDate'].min(),iit_data['VisitDate'].max())\n",
    "print(iit_data['NAD'].min(),iit_data['NAD'].max())\n",
    "\n",
    "iit_data = iit_data.drop(columns=[\n",
    "    'OptimizedHIVRegimen', 'Drug', 'VisitDate', 'WHO_Missing', 'Type',\n",
    "    'most_recent_cd4', 'regimen_switch', 'AHD', 'NAD_Imputation_Flag',\n",
    "    'BMI_Missing', 'TimeatFacility', 'Adherence_Missing', 'Facility_type_category',\n",
    "    'Pregnant_Missing', 'Breastfeeding_Missing', 'Month', 'Day'\n",
    "    # 'lastvd' to 'months_since_restart' would go here\n",
    "    # 'Month', 'Day' handled below\n",
    "    \n",
    "])\n",
    "# iit_data = iit_data.drop(columns=iit_data.loc[:, 'men_knowledge':'women_sti'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0173b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns= ['num_late_last3', 'num_late14_last3', 'num_late30_last3',\n",
    "       'num_late_last5', 'num_late14_last5', 'num_late30_last5',\n",
    "       'num_late_last10', 'num_late14_last10', 'num_late30_last10']\n",
    "iit_data[selected_columns] = iit_data[selected_columns].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b97a5508",
   "metadata": {},
   "outputs": [],
   "source": [
    "iit_data['ARTAdherence'] = iit_data['ARTAdherence'].map({\n",
    "    'good': 'optimal',\n",
    "    'fair': 'suboptimal',\n",
    "    'poor': 'suboptimal'\n",
    "}).fillna(iit_data['ARTAdherence'])\n",
    "\n",
    "# Sex: Male -> 1, else 0\n",
    "iit_data['Sex'] = (iit_data['Sex'] == 'Male').astype('Int64')\n",
    "\n",
    "# Emr: KenyaEMR -> 1, else 0\n",
    "iit_data['Emr'] = (iit_data['Emr'] == 'KenyaEMR').astype('Int64')  # assuming there is an 'Emr' colum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e9f2a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_xgboost(dataset):\n",
    "    # List of categorical variables to be encoded\n",
    "    categorical_columns = ['BMI', 'ARTAdherence', 'Pregnant', 'Breastfeeding', 'DifferentiatedCare', 'WHOStage', 'most_recent_vl', 'MaritalStatus', 'EducationLevel',\n",
    "       'Occupation', 'VisitBy','TCAReason', 'cascade_status', 'Kephlevel','Ownertype'] \n",
    "    \n",
    "    # One-hot encoding the categorical columns\n",
    "    ohe = pd.get_dummies(dataset[categorical_columns], drop_first=True, dtype=int)\n",
    "    \n",
    "    # Concatenate the original dataset (excluding categorical columns) with the one-hot encoded columns\n",
    "    dataset_encoded = pd.concat([dataset.drop(columns=categorical_columns), ohe], axis=1)\n",
    "    \n",
    "    return dataset_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a32e584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify available numeric and categorical columns\n",
    "numeric_cols = iit_data.select_dtypes(include='number').drop(columns=[\"iit\", \"SiteCode\"], errors='ignore').columns.tolist()\n",
    "categorical_cols = iit_data.select_dtypes(include='object').drop(columns=[\"key\"], errors='ignore').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4017120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "def custom_mode_imputer(column, exclude=\"NR\"):\n",
    "    counter = Counter(column.dropna())\n",
    "    if exclude in counter:\n",
    "        del counter[exclude]\n",
    "    return counter.most_common(1)[0][0] if counter else None\n",
    "\n",
    "def impute_data(df, cat_impute_values=None, num_impute_values=None, fit=False, categorical_cols=None, numeric_cols=None):\n",
    "    if fit:\n",
    "        cat_impute_values = {}\n",
    "        num_impute_values = {}\n",
    "        for col in categorical_cols:\n",
    "            cat_impute_values[col] = custom_mode_imputer(df[col], exclude=\"NR\")\n",
    "        for col in numeric_cols:\n",
    "            num_impute_values[col] = df[col].mean()\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].fillna(cat_impute_values.get(col, \"Unknown\"))\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].fillna(num_impute_values.get(col, 0))\n",
    "        \n",
    "    return df, cat_impute_values, num_impute_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "819e2a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vif_data= iit_data.copy()\n",
    "start_exclude = pd.Timestamp('2023-01-01')\n",
    "end_exclude = pd.Timestamp('2023-12-31')\n",
    "\n",
    "# Filter out records from Sept through Dec 2024\n",
    "vif_data = vif_data[~((vif_data['NAD'] >= start_exclude) & (vif_data['NAD'] <= end_exclude))]\n",
    "vif_data=vif_data.drop(columns=['key', 'SiteCode','NAD','iit'])\n",
    "vif_data, cat_impute_values, num_impute_values = impute_data(\n",
    "    vif_data,\n",
    "    fit=True,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b4ffaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6767/3042590163.py:4: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  X_numeric = vif_data.select_dtypes(include=[\"number\"]).applymap(lambda x: float(x) if isinstance(x, Decimal) else x)\n"
     ]
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "\n",
    "# Force conversion of all numeric columns to float\n",
    "X_numeric = vif_data.select_dtypes(include=[\"number\"]).applymap(lambda x: float(x) if isinstance(x, Decimal) else x)\n",
    "\n",
    "# Ensure dtype is numeric\n",
    "X_numeric = X_numeric.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Drop rows with NaNs (if any result from coercion)\n",
    "X_numeric = X_numeric.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a60161df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Feature            VIF\n",
      "0                       const  232767.227622\n",
      "1         StabilityAssessment       1.400736\n",
      "2                  FirstVisit       1.066132\n",
      "3                         Emr       1.224877\n",
      "4                         Sex       1.007300\n",
      "5                         Age       1.183629\n",
      "6       DaystoNextAppointment       1.433336\n",
      "7                   TimeonART       1.199985\n",
      "8       regimen_switch_visits       1.007879\n",
      "9            VisitUnscheduled       1.168295\n",
      "10                     lastvd       2.094608\n",
      "11             lateness_last3      30.513018\n",
      "12             num_late_last3       5.591297\n",
      "13           num_late14_last3      10.486281\n",
      "14           num_late30_last3      12.382298\n",
      "15             lateness_last5      59.557907\n",
      "16             num_late_last5       9.770060\n",
      "17           num_late14_last5      18.132832\n",
      "18           num_late30_last5      19.468604\n",
      "19            lateness_last10      25.050480\n",
      "20            num_late_last10       5.305961\n",
      "21          num_late14_last10      10.683635\n",
      "22          num_late30_last10      10.163319\n",
      "23       months_since_restart       1.546666\n",
      "24              men_knowledge       4.759015\n",
      "25            women_knowledge       2.279580\n",
      "26              men_heardaids       2.423352\n",
      "27            men_highrisksex      44.352354\n",
      "28      men_highrisksex_multi       9.527019\n",
      "29      men_sexnotwithpartner      61.887225\n",
      "30            men_sexpartners       2.831013\n",
      "31            men_nevertested       9.573730\n",
      "32           men_testedrecent      14.091835\n",
      "33                    men_sti       4.839169\n",
      "34            women_heardaids       2.428349\n",
      "35          women_highrisksex      69.856141\n",
      "36    women_highrisksex_multi       7.517113\n",
      "37    women_sexnotwithpartner      63.362270\n",
      "38          women_sexpartners       5.658984\n",
      "39          women_nevertested      11.443990\n",
      "40         women_testedrecent      11.720975\n",
      "41                  women_sti       3.995017\n",
      "42                     txcurr       1.202728\n",
      "43    rolling_weighted_noshow      43.212199\n",
      "44  rolling_weighted_dayslate      42.825065\n",
      "45                  is_friday            NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/statsmodels/regression/linear_model.py:1782: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return 1 - self.ssr/self.centered_tss\n"
     ]
    }
   ],
   "source": [
    "# Add constant and compute VIF\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X_with_const = sm.add_constant(X_numeric)\n",
    "\n",
    "vif_df = pd.DataFrame()\n",
    "vif_df[\"Feature\"] = X_with_const.columns\n",
    "vif_df[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]\n",
    "\n",
    "print(vif_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fbde0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec14ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = X_numeric.corr()\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "738febe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Variable 1                 Variable 2  Correlation\n",
      "26    rolling_weighted_noshow  rolling_weighted_dayslate     0.987446\n",
      "27  rolling_weighted_dayslate    rolling_weighted_noshow     0.987446\n",
      "24          women_highrisksex    women_sexnotwithpartner     0.977454\n",
      "25    women_sexnotwithpartner          women_highrisksex     0.977454\n",
      "21      men_sexnotwithpartner            men_highrisksex     0.941056\n",
      "20            men_highrisksex      men_sexnotwithpartner     0.941056\n",
      "8              lateness_last5            lateness_last10     0.939083\n",
      "16            lateness_last10             lateness_last5     0.939083\n",
      "7              lateness_last5             lateness_last3     0.916185\n",
      "1              lateness_last3             lateness_last5     0.916185\n",
      "9              num_late_last5             num_late_last3     0.870091\n",
      "3              num_late_last3             num_late_last5     0.870091\n",
      "4            num_late14_last3           num_late14_last5     0.856304\n",
      "11           num_late14_last5           num_late14_last3     0.856304\n",
      "2              lateness_last3            lateness_last10     0.855398\n",
      "15            lateness_last10             lateness_last3     0.855398\n",
      "17            num_late_last10             num_late_last5     0.843388\n",
      "10             num_late_last5            num_late_last10     0.843388\n",
      "6            num_late30_last3           num_late30_last5     0.838140\n",
      "13           num_late30_last5           num_late30_last3     0.838140\n",
      "12           num_late14_last5          num_late14_last10     0.827989\n",
      "18          num_late14_last10           num_late14_last5     0.827989\n",
      "5            num_late30_last3             lateness_last3     0.817277\n",
      "0              lateness_last3           num_late30_last3     0.817277\n",
      "19          num_late30_last10           num_late30_last5     0.802617\n",
      "14           num_late30_last5          num_late30_last10     0.802617\n",
      "23           men_testedrecent            men_nevertested    -0.833027\n",
      "22            men_nevertested           men_testedrecent    -0.833027\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.8\n",
    "high_corr = corr_matrix[(abs(corr_matrix) > threshold) & (abs(corr_matrix) < 1.0)]\n",
    "\n",
    "# Stack and drop NaNs to find pairs\n",
    "high_corr_pairs = high_corr.stack().reset_index()\n",
    "high_corr_pairs.columns = [\"Variable 1\", \"Variable 2\", \"Correlation\"]\n",
    "print(high_corr_pairs.sort_values(by=\"Correlation\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bffbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "iit_data= iit_data.drop(columns=['rolling_weighted_dayslate','women_sexnotwithpartner','men_sexnotwithpartner', 'men_nevertested','women_nevertested','lateness_last10',\n",
    "                                  'lateness_last5','num_late14_last5','num_late14_last10','num_late30_last5','num_late30_last10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e8911a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify available numeric and categorical columns\n",
    "numeric_cols = iit_data.select_dtypes(include='number').drop(columns=[\"iit\", \"SiteCode\"], errors='ignore').columns.tolist()\n",
    "categorical_cols = iit_data.select_dtypes(include='object').drop(columns=[\"key\"], errors='ignore').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff9b397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold 1\n",
    "train_data1 = iit_data.copy()\n",
    "train_data1 = train_data1.drop(columns=[\"SiteCode\"])\n",
    "train_data1 = train_data1[(train_data1[\"NAD\"] >= \"2023-01-01\") & (train_data1[\"NAD\"] <= \"2023-05-31\")]\n",
    "train_data1 = train_data1.drop(columns=[\"key\", \"NAD\"])\n",
    "\n",
    "train_data1, cat_impute_values, num_impute_values = impute_data(\n",
    "    train_data1,\n",
    "    fit=True,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "\n",
    "train_data1 = encode_xgboost(train_data1)\n",
    "X_train1 = train_data1.drop(columns=[\"iit\"])\n",
    "y_train1 = train_data1[\"iit\"]\n",
    "\n",
    "# ---------- Prepare Validation Data (June 2023) ----------\n",
    "val_data1 = iit_data.copy()\n",
    "val_data1 = val_data1.drop(columns=[\"SiteCode\"])\n",
    "val_data1 = val_data1[(val_data1[\"NAD\"] >= \"2023-06-01\") & (val_data1[\"NAD\"] <= \"2023-06-30\")]\n",
    "val_data1 = val_data1.drop(columns=[\"key\", \"NAD\"])\n",
    "\n",
    "val_data1, _, _ = impute_data(\n",
    "    val_data1,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "\n",
    "val_data1 = encode_xgboost(val_data1)\n",
    "X_val1 = val_data1.drop(columns=[\"iit\"])\n",
    "y_val1 = val_data1[\"iit\"]\n",
    "\n",
    "# ---------- Prepare Test Near Data (July 2023) ----------\n",
    "testnear_data1 = iit_data.copy()\n",
    "testnear_data1 = testnear_data1.drop(columns=[\"SiteCode\"])\n",
    "testnear_data1 = testnear_data1[(testnear_data1[\"NAD\"] >= \"2023-07-01\") & (testnear_data1[\"NAD\"] <= \"2023-07-31\")]\n",
    "testnear_data1 = testnear_data1.drop(columns=[\"key\", \"NAD\"])\n",
    "\n",
    "testnear_data1, _, _ = impute_data(\n",
    "    testnear_data1,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "\n",
    "testnear_data1 = encode_xgboost(testnear_data1)\n",
    "X_testnear1 = testnear_data1.drop(columns=[\"iit\"])\n",
    "y_testnear1 = testnear_data1[\"iit\"]\n",
    "\n",
    "# ---------- Prepare Test Data (July–Sept 2023) ----------\n",
    "test_data1 = iit_data.copy()\n",
    "test_data1 = test_data1.drop(columns=[\"SiteCode\"])\n",
    "test_data1 = test_data1[(test_data1[\"NAD\"] >= \"2023-07-01\") & (test_data1[\"NAD\"] <= \"2023-09-30\")]\n",
    "test_data1 = test_data1.drop(columns=[\"key\", \"NAD\"])\n",
    "\n",
    "test_data1, _, _ = impute_data(\n",
    "    test_data1,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "\n",
    "test_data1 = encode_xgboost(test_data1)\n",
    "X_test1 = test_data1.drop(columns=[\"iit\"])\n",
    "y_test1 = test_data1[\"iit\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db012aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold 2\n",
    "# Prepare Train Data (Jan–May 2023)\n",
    "train_data2 = iit_data.copy()\n",
    "train_data2 = train_data2.drop(columns=[\"SiteCode\"])\n",
    "train_data2 = train_data2[(train_data2[\"NAD\"] >= \"2023-04-01\") & (train_data2[\"NAD\"] <= \"2023-08-31\")]\n",
    "train_data2 = train_data2.drop(columns=[\"key\", \"NAD\"])\n",
    "\n",
    "# Impute train data and get imputation values\n",
    "train_data2, cat_impute_values, num_impute_values = impute_data(\n",
    "    train_data2,\n",
    "    fit=True,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "\n",
    "train_data2 = encode_xgboost(train_data2)\n",
    "\n",
    "X_train2 = train_data2.drop(columns=[\"iit\"])\n",
    "y_train2 = train_data2[\"iit\"]\n",
    "\n",
    "# Validation Data (June 2023)\n",
    "val_data2 = iit_data.copy()\n",
    "val_data2 = val_data2.drop(columns=[\"SiteCode\"])\n",
    "val_data2 = val_data2[(val_data2[\"NAD\"] >= \"2023-09-01\") & (val_data2[\"NAD\"] <= \"2023-09-30\")]\n",
    "val_data2 = val_data2.drop(columns=[\"key\", \"NAD\"])\n",
    "val_data2, _, _ = impute_data(\n",
    "    val_data2,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "val_data2 = encode_xgboost(val_data2)\n",
    "X_val2 = val_data2.drop(columns=[\"iit\"])\n",
    "y_val2 = val_data2[\"iit\"]\n",
    "\n",
    "\n",
    "# Test Near Data (July 2023)\n",
    "testnear_data2 = iit_data.copy()\n",
    "testnear_data2 = testnear_data2.drop(columns=[\"SiteCode\"])\n",
    "testnear_data2 = testnear_data2[(testnear_data2[\"NAD\"] >= \"2023-10-01\") & (testnear_data2[\"NAD\"] <= \"2023-10-31\")]\n",
    "testnear_data2 = testnear_data2.drop(columns=[\"key\", \"NAD\"])\n",
    "testnear_data2, _, _ = impute_data(\n",
    "    testnear_data2,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "testnear_data2 = encode_xgboost(testnear_data2)\n",
    "\n",
    "X_testnear2 = testnear_data2.drop(columns=[\"iit\"])\n",
    "y_testnear2 = testnear_data2[\"iit\"]\n",
    "\n",
    "# Test Data (July–Sept 2023)\n",
    "test_data2 = iit_data.copy()\n",
    "test_data2 = test_data2.drop(columns=[\"SiteCode\"])\n",
    "test_data2 = test_data2[(test_data2[\"NAD\"] >= \"2023-10-01\") & (test_data2[\"NAD\"] <= \"2023-12-31\")]\n",
    "test_data2 = test_data2.drop(columns=[\"key\", \"NAD\"])\n",
    "test_data2, _, _ = impute_data(\n",
    "    test_data2,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "test_data2 = encode_xgboost(test_data2)\n",
    "\n",
    "X_test2 = test_data2.drop(columns=[\"iit\"])\n",
    "y_test2 = test_data2[\"iit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec594c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold 3\n",
    "# Prepare Train Data (Jan–May 3033)\n",
    "train_data3 = iit_data.copy()\n",
    "train_data3 = train_data3.drop(columns=[\"SiteCode\"])\n",
    "train_data3 = train_data3[(train_data3[\"NAD\"] >= \"2023-06-01\") & (train_data3[\"NAD\"] <= \"2023-11-30\")]\n",
    "train_data3 = train_data3.drop(columns=[\"key\", \"NAD\"])\n",
    "\n",
    "# Impute train data and get imputation values\n",
    "train_data3, cat_impute_values, num_impute_values = impute_data(\n",
    "    train_data3,\n",
    "    fit=True,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "\n",
    "train_data3 = encode_xgboost(train_data3)\n",
    "\n",
    "X_train3 = train_data3.drop(columns=[\"iit\"])\n",
    "y_train3 = train_data3[\"iit\"]\n",
    "\n",
    "# Validation Data (June 3033)\n",
    "val_data3 = iit_data.copy()\n",
    "val_data3 = val_data3.drop(columns=[\"SiteCode\"])\n",
    "val_data3 = val_data3[(val_data3[\"NAD\"] >= \"2023-12-01\") & (val_data3[\"NAD\"] <= \"2023-12-31\")]\n",
    "val_data3 = val_data3.drop(columns=[\"key\", \"NAD\"])\n",
    "val_data3, _, _ = impute_data(\n",
    "    val_data3,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "val_data3 = encode_xgboost(val_data3)\n",
    "\n",
    "X_val3 = val_data3.drop(columns=[\"iit\"])\n",
    "y_val3 = val_data3[\"iit\"]\n",
    "# Test Near Data (July 3033)\n",
    "testnear_data3 = iit_data.copy()\n",
    "testnear_data3 = testnear_data3.drop(columns=[\"SiteCode\"])\n",
    "testnear_data3 = testnear_data3[(testnear_data3[\"NAD\"] >= \"2024-01-01\") & (testnear_data3[\"NAD\"] <= \"2024-01-31\")]\n",
    "testnear_data3 = testnear_data3.drop(columns=[\"key\", \"NAD\"])\n",
    "testnear_data3, _, _ = impute_data(\n",
    "    testnear_data3,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "testnear_data3 = encode_xgboost(testnear_data3)\n",
    "\n",
    "X_testnear3 = testnear_data3.drop(columns=[\"iit\"])\n",
    "y_testnear3 = testnear_data3[\"iit\"]\n",
    "\n",
    "# Test Data (July–Sept 3033)\n",
    "test_data3 = iit_data.copy()\n",
    "test_data3 = test_data3.drop(columns=[\"SiteCode\"])\n",
    "test_data3 = test_data3[(test_data3[\"NAD\"] >= \"2024-01-01\") & (test_data3[\"NAD\"] <= \"2024-01-31\")]\n",
    "test_data3 = test_data3.drop(columns=[\"key\", \"NAD\"])\n",
    "test_data3, _, _ = impute_data(\n",
    "    test_data3,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "test_data3 = encode_xgboost(test_data3)\n",
    "\n",
    "X_test3 = test_data3.drop(columns=[\"iit\"])\n",
    "y_test3 = test_data3[\"iit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b65152b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold 4\n",
    "# Prepare Train Data (Jan–May 4044)\n",
    "train_data4 = iit_data.copy()\n",
    "train_data4 = train_data4.drop(columns=[\"SiteCode\"])\n",
    "train_data4 = train_data4[(train_data4[\"NAD\"] >= \"2023-09-01\") & (train_data4[\"NAD\"] <= \"2024-02-29\")]\n",
    "train_data4 = train_data4.drop(columns=[\"key\", \"NAD\"])\n",
    "\n",
    "# Impute train data and get imputation values\n",
    "train_data4, cat_impute_values, num_impute_values = impute_data(\n",
    "    train_data4,\n",
    "    fit=True,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "\n",
    "train_data4 = encode_xgboost(train_data4)\n",
    "\n",
    "X_train4 = train_data4.drop(columns=[\"iit\"])\n",
    "y_train4 = train_data4[\"iit\"]\n",
    "\n",
    "# Validation Data (June 4044)\n",
    "val_data4 = iit_data.copy()\n",
    "val_data4 = val_data4.drop(columns=[\"SiteCode\"])\n",
    "val_data4 = val_data4[(val_data4[\"NAD\"] >= \"2024-03-01\") & (val_data4[\"NAD\"] <= \"2024-03-31\")]\n",
    "val_data4 = val_data4.drop(columns=[\"key\", \"NAD\"])\n",
    "val_data4, _, _ = impute_data(\n",
    "    val_data4,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "val_data4 = encode_xgboost(val_data4)\n",
    "\n",
    "X_val4 = val_data4.drop(columns=[\"iit\"])\n",
    "y_val4 = val_data4[\"iit\"]\n",
    "# Test Near Data (July 4044)\n",
    "testnear_data4 = iit_data.copy()\n",
    "testnear_data4 = testnear_data4.drop(columns=[\"SiteCode\"])\n",
    "testnear_data4 = testnear_data4[(testnear_data4[\"NAD\"] >= \"2024-04-01\") & (testnear_data4[\"NAD\"] <= \"2024-04-30\")]\n",
    "testnear_data4 = testnear_data4.drop(columns=[\"key\", \"NAD\"])\n",
    "testnear_data4, _, _ = impute_data(\n",
    "    testnear_data4,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "testnear_data4 = encode_xgboost(testnear_data4)\n",
    "X_testnear4 = testnear_data4.drop(columns=[\"iit\"])\n",
    "y_testnear4 = testnear_data4[\"iit\"]\n",
    "\n",
    "\n",
    "# Test Data (July–Sept 4044)\n",
    "test_data4 = iit_data.copy()\n",
    "test_data4 = test_data4.drop(columns=[\"SiteCode\"])\n",
    "test_data4 = test_data4[(test_data4[\"NAD\"] >= \"2024-04-01\") & (test_data4[\"NAD\"] <= \"2024-06-30\")]\n",
    "test_data4 = test_data4.drop(columns=[\"key\", \"NAD\"])\n",
    "test_data4, _, _ = impute_data(\n",
    "    test_data4,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "test_data4 = encode_xgboost(test_data4)\n",
    "\n",
    "X_test4 = test_data4.drop(columns=[\"iit\"])\n",
    "y_test4 = test_data4[\"iit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4be59a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold 5\n",
    "# Prepare Train Data (Jan–May 5055)\n",
    "train_data5 = iit_data.copy()\n",
    "train_data5 = train_data5.drop(columns=[\"SiteCode\"])\n",
    "train_data5 = train_data5[(train_data5[\"NAD\"] >= \"2024-01-01\") & (train_data5[\"NAD\"] <= \"2024-05-31\")]\n",
    "train_data5 = train_data5.drop(columns=[\"key\", \"NAD\"])\n",
    "\n",
    "# Impute train data and get imputation values\n",
    "train_data5, cat_impute_values, num_impute_values = impute_data(\n",
    "    train_data5,\n",
    "    fit=True,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "\n",
    "train_data5 = encode_xgboost(train_data5)\n",
    "X_train5 = train_data5.drop(columns=[\"iit\"])\n",
    "y_train5 = train_data5[\"iit\"]\n",
    "\n",
    "\n",
    "# Validation Data (June 5055)\n",
    "val_data5 = iit_data.copy()\n",
    "val_data5 = val_data5.drop(columns=[\"SiteCode\"])\n",
    "val_data5 = val_data5[(val_data5[\"NAD\"] >= \"2024-06-01\") & (val_data5[\"NAD\"] <= \"2024-06-30\")]\n",
    "val_data5 = val_data5.drop(columns=[\"key\", \"NAD\"])\n",
    "val_data5, _, _ = impute_data(\n",
    "    val_data5,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "val_data5 = encode_xgboost(val_data5)\n",
    "\n",
    "X_val5 = val_data5.drop(columns=[\"iit\"])\n",
    "y_val5 = val_data5[\"iit\"]\n",
    "# Test Near Data (July 5055)\n",
    "testnear_data5 = iit_data.copy()\n",
    "testnear_data5 = testnear_data5.drop(columns=[\"SiteCode\"])\n",
    "testnear_data5 = testnear_data5[(testnear_data5[\"NAD\"] >= \"2024-07-01\") & (testnear_data5[\"NAD\"] <= \"2024-07-31\")]\n",
    "testnear_data5 = testnear_data5.drop(columns=[\"key\", \"NAD\"])\n",
    "testnear_data5, _, _ = impute_data(\n",
    "    testnear_data5,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "testnear_data5 = encode_xgboost(testnear_data5)\n",
    "\n",
    "X_testnear5 = testnear_data5.drop(columns=[\"iit\"])\n",
    "y_testnear5 = testnear_data5[\"iit\"]\n",
    "\n",
    "# Test Data (July–Sept 5055)\n",
    "test_data5 = iit_data.copy()\n",
    "test_data5 = test_data5.drop(columns=[\"SiteCode\"])\n",
    "test_data5 = test_data5[(test_data5[\"NAD\"] >= \"2024-07-01\") & (test_data5[\"NAD\"] <= \"2024-09-30\")]\n",
    "test_data5 = test_data5.drop(columns=[\"key\", \"NAD\"])\n",
    "test_data5, _, _ = impute_data(\n",
    "    test_data5,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "test_data5 = encode_xgboost(test_data5)\n",
    "\n",
    "X_test5 = test_data5.drop(columns=[\"iit\"])\n",
    "y_test5 = test_data5[\"iit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87264b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_list = {\n",
    "    \"fold1\": [X_train1, y_train1, X_val1, y_val1, X_testnear1, y_testnear1,X_test1,y_test1],\n",
    "    \"fold2\": [X_train2, y_train2, X_val2, y_val2, X_testnear2, y_testnear2,X_test2,y_test2],\n",
    "    \"fold3\": [X_train3, y_train3, X_val3, y_val3, X_testnear3, y_testnear3, X_test3,y_test3],\n",
    "    \"fold4\": [X_train4, y_train4, X_val4, y_val4, X_testnear4, y_testnear4,X_test4,y_test4],\n",
    "    \"fold5\": [X_train5, y_train5, X_val5, y_val5, X_testnear5, y_testnear5, X_test5,y_test5],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fa81613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Create a grid for Logistic Regression hyperparameters\n",
    "params_grid = list(product(\n",
    "    ['l2'],             # penalty\n",
    "    [1.0] ,       # inverse of regularization strength C\n",
    "    ['liblinear']  # Add solver\n",
    "))\n",
    "\n",
    "# Build DataFrame\n",
    "grid_sparse = pd.DataFrame(params_grid, columns=[\n",
    "    \"penalty\", \"C\",\"solver\"\n",
    "])\n",
    "\n",
    "# Step 2: Add empty columns to record validation metrics (PR AUC)\n",
    "for k in range(1, 6):\n",
    "    grid_sparse[f\"val_pr_auc_near_{k}\"] = np.nan\n",
    "    grid_sparse[f\"val_pr_auc_{k}\"] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "466bb413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>penalty</th>\n",
       "      <th>C</th>\n",
       "      <th>solver</th>\n",
       "      <th>val_pr_auc_near_1</th>\n",
       "      <th>val_pr_auc_1</th>\n",
       "      <th>val_pr_auc_near_2</th>\n",
       "      <th>val_pr_auc_2</th>\n",
       "      <th>val_pr_auc_near_3</th>\n",
       "      <th>val_pr_auc_3</th>\n",
       "      <th>val_pr_auc_near_4</th>\n",
       "      <th>val_pr_auc_4</th>\n",
       "      <th>val_pr_auc_near_5</th>\n",
       "      <th>val_pr_auc_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>l2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  penalty    C     solver  val_pr_auc_near_1  val_pr_auc_1  val_pr_auc_near_2  \\\n",
       "0      l2  1.0  liblinear                NaN           NaN                NaN   \n",
       "\n",
       "   val_pr_auc_2  val_pr_auc_near_3  val_pr_auc_3  val_pr_auc_near_4  \\\n",
       "0           NaN                NaN           NaN                NaN   \n",
       "\n",
       "   val_pr_auc_4  val_pr_auc_near_5  val_pr_auc_5  \n",
       "0           NaN                NaN           NaN  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8932897e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search: 100%|██████████| 1/1 [45:16<00:00, 2716.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penalty                     l2\n",
      "C                          1.0\n",
      "solver               liblinear\n",
      "val_pr_auc_near_1     0.122133\n",
      "val_pr_auc_1          0.129589\n",
      "val_pr_auc_near_2     0.147689\n",
      "val_pr_auc_2          0.133898\n",
      "val_pr_auc_near_3     0.123767\n",
      "val_pr_auc_3          0.123767\n",
      "val_pr_auc_near_4     0.099695\n",
      "val_pr_auc_4          0.096408\n",
      "val_pr_auc_near_5     0.109903\n",
      "val_pr_auc_5          0.110506\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(grid_sparse)), desc=\"Grid Search\"):\n",
    "\n",
    "    row = grid_sparse.iloc[i]\n",
    "\n",
    "    for k in tqdm(range(1, 6), desc=f\"Fold {i+1}\", leave=False):\n",
    "\n",
    "        # Unpack fold-specific data\n",
    "        X_train, y_train, X_val, y_val, X_testnear, y_testnear, X_test, y_test = fold_list[f\"fold{k}\"]\n",
    "\n",
    "        try:\n",
    "            # Initialize Logistic Regression model\n",
    "            model = LogisticRegression(\n",
    "                penalty=row[\"penalty\"],\n",
    "                C=row[\"C\"],\n",
    "                solver=row[\"solver\"],\n",
    "                class_weight='balanced',\n",
    "                max_iter=100,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            # Fit the model\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Predict on near test set\n",
    "            testnear_preds = model.predict_proba(X_testnear)[:, 1]\n",
    "            ap_near = average_precision_score(y_testnear, testnear_preds)\n",
    "            grid_sparse.at[grid_sparse.index[i], f\"val_pr_auc_near_{k}\"] = ap_near\n",
    "\n",
    "            # Predict on main test set\n",
    "            test_preds = model.predict_proba(X_test)[:, 1]\n",
    "            ap = average_precision_score(y_test, test_preds)\n",
    "            grid_sparse.at[grid_sparse.index[i], f\"val_pr_auc_{k}\"] = ap\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed on grid index {i} fold {k}: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(grid_sparse.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bddb653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume y_true and y_pred_proba are already defined\n",
    "test_preds = model.predict_proba(X_test5)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test5, test_preds)\n",
    "roc_auc = roc_auc_score(y_test5, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "102c288d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7381695188528021)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbbe847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'W0AZSKZV9V2M9NQX',\n",
       "  'HostId': 'KGoPWPTCrOqXUt41RmncK6nRQdCyIGJAQQOvyV2V2ICDNiB/5FqvrMGN/FgvjY0ZGBs2bFN2hK0=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'KGoPWPTCrOqXUt41RmncK6nRQdCyIGJAQQOvyV2V2ICDNiB/5FqvrMGN/FgvjY0ZGBs2bFN2hK0=',\n",
       "   'x-amz-request-id': 'W0AZSKZV9V2M9NQX',\n",
       "   'date': 'Tue, 13 May 2025 12:26:18 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"3af81b0aa122cc8d62dc49c3d36fce7d\"',\n",
       "   'x-amz-checksum-crc32': 'kpOazg==',\n",
       "   'x-amz-checksum-type': 'FULL_OBJECT',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"3af81b0aa122cc8d62dc49c3d36fce7d\"',\n",
       " 'ChecksumCRC32': 'kpOazg==',\n",
       " 'ChecksumType': 'FULL_OBJECT',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "s3 = boto3.client('s3')  # assumes you've run aws configure or have IAM role\n",
    "# Create a CSV in memory\n",
    "csv_buffer = StringIO()\n",
    "test.to_csv(csv_buffer, index=False)\n",
    "s3.put_object(\n",
    "    Bucket='kehmisjan2025',\n",
    "    Key='test_051325_rf.csv',\n",
    "    Body=csv_buffer.getvalue()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
