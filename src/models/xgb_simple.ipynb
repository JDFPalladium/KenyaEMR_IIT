{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import boto3\n",
    "import tempfile\n",
    "from settings import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285af4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID'] = settings.AWS_ACCESS_KEY_ID\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = settings.AWS_SECRET_ACCESS_KEY\n",
    "os.environ['AWS_DEFAULT_REGION'] = settings.AWS_DEFAULT_REGION\n",
    "\n",
    "# Define S3 info\n",
    "bucket_name = 'kehmisjan2025'\n",
    "file_key = 'targets_apr2.rds'\n",
    "\n",
    "# Initialize boto3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Download to a temporary file\n",
    "with tempfile.NamedTemporaryFile(suffix=\".rds\") as tmp_file:\n",
    "    s3.download_fileobj(bucket_name, file_key, tmp_file)\n",
    "    tmp_file.seek(0)  # go back to beginning\n",
    "    result = pyreadr.read_r(tmp_file.name)  # returns a dictionary\n",
    "\n",
    "# Extract the data frame\n",
    "iit_data = next(iter(result.values()))  # assumes only one object inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15124ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(iit_data.dtypes) \n",
    "# Ensure the 'NAD' column is converted to datetime\n",
    "iit_data['NAD'] = pd.to_datetime(iit_data['NAD'], format='%Y-%m-%d')\n",
    "# iit_data['VisitDate'] = pd.to_datetime(iit_data['VisitDate'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a5fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the last quarter of the year\n",
    "# Define the date range to exclude\n",
    "start_exclude = pd.Timestamp('2024-10-02')\n",
    "end_exclude = pd.Timestamp('2024-12-31')\n",
    "\n",
    "# Filter out records from Sept through Dec 2024\n",
    "iit_data = iit_data[~((iit_data['NAD'] >= start_exclude) & (iit_data['NAD'] <= end_exclude))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1747f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start date (January 2022)\n",
    "start_date = pd.to_datetime('2022-01-01')\n",
    "\n",
    "# Calculate 'tp' as the difference in months from January 2022\n",
    "iit_data['tp'] = ((iit_data['NAD'].dt.year - start_date.year) * 12) + (iit_data['NAD'].dt.month - start_date.month) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f79ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iit_data['is_december'] = iit_data['Month'].apply(lambda x: 1 if x == \"December\" else 0)\n",
    "iit_data['is_friday'] = iit_data['Day'].apply(lambda x: 1 if x == \"Friday\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9f0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "iit_data['BMI'] = np.where(iit_data['BMI'].isna() & (iit_data['BMI_Missing'] == 0), 'NR', iit_data['BMI'])\n",
    "iit_data['ARTAdherence'] = np.where(iit_data['ARTAdherence'].isna() & (iit_data['Adherence_Missing'] == 0), 'NR', iit_data['ARTAdherence'])\n",
    "iit_data['Pregnant'] = np.where(iit_data['Pregnant'].isna() & (iit_data['Pregnant_Missing'] == 0), 'NR', iit_data['Pregnant'])\n",
    "iit_data['Breastfeeding'] = np.where(iit_data['Breastfeeding'].isna() & (iit_data['Breastfeeding_Missing'] == 0), 'NR', iit_data['Breastfeeding'])\n",
    "iit_data['WHOStage'] = np.where(iit_data['WHOStage'].isna() & (iit_data['WHO_Missing'] == 0), 'NR', iit_data['WHOStage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f5a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iit_data['VisitDate'].min(),iit_data['VisitDate'].max())\n",
    "print(iit_data['NAD'].min(),iit_data['NAD'].max())\n",
    "\n",
    "iit_data = iit_data.drop(columns=[\n",
    "    'OptimizedHIVRegimen', 'Drug', 'VisitDate', 'WHO_Missing', 'Type',\n",
    "    'most_recent_cd4', 'regimen_switch', 'AHD', 'NAD_Imputation_Flag',\n",
    "    'BMI_Missing', 'TimeatFacility', 'Adherence_Missing', 'Facility_type_category',\n",
    "    'Pregnant_Missing', 'Breastfeeding_Missing', 'Month', 'Day'\n",
    "    # 'lastvd' to 'months_since_restart' would go here\n",
    "    # 'Month', 'Day' handled below\n",
    "    \n",
    "])\n",
    "# iit_data = iit_data.drop(columns=iit_data.loc[:, 'men_knowledge':'women_sti'].columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns= ['num_late_last3', 'num_late14_last3', 'num_late30_last3',\n",
    "       'num_late_last5', 'num_late14_last5', 'num_late30_last5',\n",
    "       'num_late_last10', 'num_late14_last10', 'num_late30_last10']\n",
    "iit_data[selected_columns] = iit_data[selected_columns].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88afe0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "iit_data['ARTAdherence'] = iit_data['ARTAdherence'].map({\n",
    "    'good': 'optimal',\n",
    "    'fair': 'suboptimal',\n",
    "    'poor': 'suboptimal'\n",
    "}).fillna(iit_data['ARTAdherence'])\n",
    "\n",
    "# Sex: Male -> 1, else 0\n",
    "iit_data['Sex'] = (iit_data['Sex'] == 'Male').astype('Int64')\n",
    "\n",
    "# Emr: KenyaEMR -> 1, else 0\n",
    "iit_data['Emr'] = (iit_data['Emr'] == 'KenyaEMR').astype('Int64')  # assuming there is an 'Emr' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c6f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_xgboost(dataset):\n",
    "    # List of categorical variables to be encoded\n",
    "    categorical_columns = ['BMI', 'ARTAdherence', 'Pregnant', 'Breastfeeding', 'DifferentiatedCare', 'WHOStage', 'most_recent_vl', 'MaritalStatus', 'EducationLevel',\n",
    "       'Occupation', 'VisitBy','TCAReason', 'cascade_status', 'Kephlevel','Ownertype'] \n",
    "    \n",
    "    # One-hot encoding the categorical columns\n",
    "    ohe = pd.get_dummies(dataset[categorical_columns], drop_first=True, dtype=int)\n",
    "    \n",
    "    # Concatenate the original dataset (excluding categorical columns) with the one-hot encoded columns\n",
    "    dataset_encoded = pd.concat([dataset.drop(columns=categorical_columns), ohe], axis=1)\n",
    "    \n",
    "    return dataset_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4c298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify available numeric and categorical columns\n",
    "numeric_cols = iit_data.select_dtypes(include='number').drop(columns=[\"iit\"], errors='ignore').columns.tolist()\n",
    "categorical_cols = iit_data.select_dtypes(include='object').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d9d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n",
    "\n",
    "def custom_mode_imputer(column, exclude=\"NR\"):\n",
    "    counter = Counter(column.dropna())\n",
    "    if exclude in counter:\n",
    "        del counter[exclude]\n",
    "    return counter.most_common(1)[0][0] if counter else None\n",
    "\n",
    "def impute_data(df, cat_impute_values=None, num_impute_values=None, fit=False, categorical_cols=None, numeric_cols=None):\n",
    "    if fit:\n",
    "        cat_impute_values = {}\n",
    "        num_impute_values = {}\n",
    "        for col in categorical_cols:\n",
    "            cat_impute_values[col] = custom_mode_imputer(df[col], exclude=\"NR\")\n",
    "        for col in numeric_cols:\n",
    "            num_impute_values[col] = df[col].mean()\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].fillna(cat_impute_values.get(col, \"Unknown\"))\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].fillna(num_impute_values.get(col, 0))\n",
    "        \n",
    "    return df, cat_impute_values, num_impute_values\n",
    "\n",
    "\n",
    "# Prepare Train Data (Janâ€“May 2023)\n",
    "train_data1 = iit_data.copy()\n",
    "train_data1 = train_data1.drop(columns=[\"SiteCode\"])\n",
    "train_data1 = train_data1[(train_data1[\"NAD\"] >= \"2023-01-01\") & (train_data1[\"NAD\"] <= \"2023-05-31\")]\n",
    "train_data1 = train_data1.drop(columns=[\"key\", \"NAD\"])\n",
    "\n",
    "# Impute train data and get imputation values\n",
    "train_data1, cat_impute_values, num_impute_values = impute_data(\n",
    "    train_data1,\n",
    "    fit=True,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "\n",
    "train_data1 = encode_xgboost(train_data1)\n",
    "\n",
    "dtrain1 = xgb.DMatrix(\n",
    "    data=train_data1.drop(columns=[\"iit\"]),\n",
    "    label=train_data1[\"iit\"]\n",
    ")\n",
    "\n",
    "# Validation Data (June 2023)\n",
    "val_data1 = iit_data.copy()\n",
    "val_data1 = val_data1.drop(columns=[\"SiteCode\"])\n",
    "val_data1 = val_data1[(val_data1[\"NAD\"] >= \"2023-06-01\") & (val_data1[\"NAD\"] <= \"2023-06-30\")]\n",
    "val_data1 = val_data1.drop(columns=[\"key\", \"NAD\"])\n",
    "val_data1, _, _ = impute_data(\n",
    "    val_data1,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "val_data1 = encode_xgboost(val_data1)\n",
    "\n",
    "dval1 = xgb.DMatrix(\n",
    "    data=val_data1.drop(columns=[\"iit\"]),\n",
    "    label=val_data1[\"iit\"]\n",
    ")\n",
    "\n",
    "# Test Near Data (July 2023)\n",
    "testnear_data1 = iit_data.copy()\n",
    "testnear_data1 = testnear_data1.drop(columns=[\"SiteCode\"])\n",
    "testnear_data1 = testnear_data1[(testnear_data1[\"NAD\"] >= \"2023-07-01\") & (testnear_data1[\"NAD\"] <= \"2023-07-31\")]\n",
    "testnear_data1 = testnear_data1.drop(columns=[\"key\", \"NAD\"])\n",
    "testnear_data1, _, _ = impute_data(\n",
    "    testnear_data1,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "testnear_data1 = encode_xgboost(testnear_data1)\n",
    "\n",
    "dtestnear1 = xgb.DMatrix(\n",
    "    data=testnear_data1.drop(columns=[\"iit\"]),\n",
    "    label=testnear_data1[\"iit\"]\n",
    ")\n",
    "\n",
    "# Test Data (Julyâ€“Sept 2023)\n",
    "test_data1 = iit_data.copy()\n",
    "test_data1 = test_data1.drop(columns=[\"SiteCode\"])\n",
    "test_data1 = test_data1[(test_data1[\"NAD\"] >= \"2023-07-01\") & (test_data1[\"NAD\"] <= \"2023-09-30\")]\n",
    "test_data1 = test_data1.drop(columns=[\"key\", \"NAD\"])\n",
    "test_data1, _, _ = impute_data(\n",
    "    test_data1,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "test_data1 = encode_xgboost(test_data1)\n",
    "\n",
    "dtest1 = xgb.DMatrix(\n",
    "    data=test_data1.drop(columns=[\"iit\"]),\n",
    "    label=test_data1[\"iit\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold 2\n",
    "# Prepare Train Data (Janâ€“May 2023)\n",
    "train_data2 = iit_data.copy()\n",
    "train_data2 = train_data2.drop(columns=[\"SiteCode\"])\n",
    "train_data2 = train_data2[(train_data2[\"NAD\"] >= \"2023-04-01\") & (train_data2[\"NAD\"] <= \"2023-08-31\")]\n",
    "train_data2 = train_data2.drop(columns=[\"key\", \"NAD\"])\n",
    "\n",
    "# Impute train data and get imputation values\n",
    "train_data2, cat_impute_values, num_impute_values = impute_data(\n",
    "    train_data2,\n",
    "    fit=True,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "\n",
    "train_data2 = encode_xgboost(train_data2)\n",
    "\n",
    "dtrain2 = xgb.DMatrix(\n",
    "    data=train_data2.drop(columns=[\"iit\"]),\n",
    "    label=train_data2[\"iit\"]\n",
    ")\n",
    "\n",
    "# Validation Data (June 2023)\n",
    "val_data2 = iit_data.copy()\n",
    "val_data2 = val_data2.drop(columns=[\"SiteCode\"])\n",
    "val_data2 = val_data2[(val_data2[\"NAD\"] >= \"2023-09-01\") & (val_data2[\"NAD\"] <= \"2023-09-30\")]\n",
    "val_data2 = val_data2.drop(columns=[\"key\", \"NAD\"])\n",
    "val_data2, _, _ = impute_data(\n",
    "    val_data2,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "val_data2 = encode_xgboost(val_data2)\n",
    "\n",
    "dval2 = xgb.DMatrix(\n",
    "    data=val_data2.drop(columns=[\"iit\"]),\n",
    "    label=val_data2[\"iit\"]\n",
    ")\n",
    "\n",
    "# Test Near Data (July 2023)\n",
    "testnear_data2 = iit_data.copy()\n",
    "testnear_data2 = testnear_data2.drop(columns=[\"SiteCode\"])\n",
    "testnear_data2 = testnear_data2[(testnear_data2[\"NAD\"] >= \"2023-10-01\") & (testnear_data2[\"NAD\"] <= \"2023-10-31\")]\n",
    "testnear_data2 = testnear_data2.drop(columns=[\"key\", \"NAD\"])\n",
    "testnear_data2, _, _ = impute_data(\n",
    "    testnear_data2,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "testnear_data2 = encode_xgboost(testnear_data2)\n",
    "\n",
    "dtestnear2 = xgb.DMatrix(\n",
    "    data=testnear_data2.drop(columns=[\"iit\"]),\n",
    "    label=testnear_data2[\"iit\"]\n",
    ")\n",
    "\n",
    "# Test Data (Julyâ€“Sept 2023)\n",
    "test_data2 = iit_data.copy()\n",
    "test_data2 = test_data2.drop(columns=[\"SiteCode\"])\n",
    "test_data2 = test_data2[(test_data2[\"NAD\"] >= \"2023-10-01\") & (test_data2[\"NAD\"] <= \"2023-12-31\")]\n",
    "test_data2 = test_data2.drop(columns=[\"key\", \"NAD\"])\n",
    "test_data2, _, _ = impute_data(\n",
    "    test_data2,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "test_data2 = encode_xgboost(test_data2)\n",
    "\n",
    "dtest2 = xgb.DMatrix(\n",
    "    data=test_data2.drop(columns=[\"iit\"]),\n",
    "    label=test_data2[\"iit\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edff6e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold 3\n",
    "# Prepare Train Data (Janâ€“May 3033)\n",
    "train_data3 = iit_data.copy()\n",
    "train_data3 = train_data3.drop(columns=[\"SiteCode\"])\n",
    "train_data3 = train_data3[(train_data3[\"NAD\"] >= \"2023-06-01\") & (train_data3[\"NAD\"] <= \"2023-11-30\")]\n",
    "train_data3 = train_data3.drop(columns=[\"key\", \"NAD\"])\n",
    "\n",
    "# Impute train data and get imputation values\n",
    "train_data3, cat_impute_values, num_impute_values = impute_data(\n",
    "    train_data3,\n",
    "    fit=True,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "\n",
    "train_data3 = encode_xgboost(train_data3)\n",
    "\n",
    "dtrain3 = xgb.DMatrix(\n",
    "    data=train_data3.drop(columns=[\"iit\"]),\n",
    "    label=train_data3[\"iit\"]\n",
    ")\n",
    "\n",
    "# Validation Data (June 3033)\n",
    "val_data3 = iit_data.copy()\n",
    "val_data3 = val_data3.drop(columns=[\"SiteCode\"])\n",
    "val_data3 = val_data3[(val_data3[\"NAD\"] >= \"2023-12-01\") & (val_data3[\"NAD\"] <= \"2023-12-31\")]\n",
    "val_data3 = val_data3.drop(columns=[\"key\", \"NAD\"])\n",
    "val_data3, _, _ = impute_data(\n",
    "    val_data3,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "val_data3 = encode_xgboost(val_data3)\n",
    "\n",
    "dval3 = xgb.DMatrix(\n",
    "    data=val_data3.drop(columns=[\"iit\"]),\n",
    "    label=val_data3[\"iit\"]\n",
    ")\n",
    "# Test Near Data (July 3033)\n",
    "testnear_data3 = iit_data.copy()\n",
    "testnear_data3 = testnear_data3.drop(columns=[\"SiteCode\"])\n",
    "testnear_data3 = testnear_data3[(testnear_data3[\"NAD\"] >= \"2024-01-01\") & (testnear_data3[\"NAD\"] <= \"2024-01-31\")]\n",
    "testnear_data3 = testnear_data3.drop(columns=[\"key\", \"NAD\"])\n",
    "testnear_data3, _, _ = impute_data(\n",
    "    testnear_data3,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "testnear_data3 = encode_xgboost(testnear_data3)\n",
    "\n",
    "dtestnear3 = xgb.DMatrix(\n",
    "    data=testnear_data3.drop(columns=[\"iit\"]),\n",
    "    label=testnear_data3[\"iit\"]\n",
    ")\n",
    "\n",
    "# Test Data (Julyâ€“Sept 3033)\n",
    "test_data3 = iit_data.copy()\n",
    "test_data3 = test_data3.drop(columns=[\"SiteCode\"])\n",
    "test_data3 = test_data3[(test_data3[\"NAD\"] >= \"2024-01-01\") & (test_data3[\"NAD\"] <= \"2024-01-31\")]\n",
    "test_data3 = test_data3.drop(columns=[\"key\", \"NAD\"])\n",
    "test_data3, _, _ = impute_data(\n",
    "    test_data3,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "test_data3 = encode_xgboost(test_data3)\n",
    "\n",
    "dtest3 = xgb.DMatrix(\n",
    "    data=test_data3.drop(columns=[\"iit\"]),\n",
    "    label=test_data3[\"iit\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20320366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold 4\n",
    "# Prepare Train Data (Janâ€“May 4044)\n",
    "train_data4 = iit_data.copy()\n",
    "train_data4 = train_data4.drop(columns=[\"SiteCode\"])\n",
    "train_data4 = train_data4[(train_data4[\"NAD\"] >= \"2023-09-01\") & (train_data4[\"NAD\"] <= \"2024-02-29\")]\n",
    "train_data4 = train_data4.drop(columns=[\"key\", \"NAD\"])\n",
    "\n",
    "# Impute train data and get imputation values\n",
    "train_data4, cat_impute_values, num_impute_values = impute_data(\n",
    "    train_data4,\n",
    "    fit=True,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "\n",
    "train_data4 = encode_xgboost(train_data4)\n",
    "\n",
    "dtrain4 = xgb.DMatrix(\n",
    "    data=train_data4.drop(columns=[\"iit\"]),\n",
    "    label=train_data4[\"iit\"]\n",
    ")\n",
    "\n",
    "# Validation Data (June 4044)\n",
    "val_data4 = iit_data.copy()\n",
    "val_data4 = val_data4.drop(columns=[\"SiteCode\"])\n",
    "val_data4 = val_data4[(val_data4[\"NAD\"] >= \"2024-03-01\") & (val_data4[\"NAD\"] <= \"2024-03-31\")]\n",
    "val_data4 = val_data4.drop(columns=[\"key\", \"NAD\"])\n",
    "val_data4, _, _ = impute_data(\n",
    "    val_data4,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "val_data4 = encode_xgboost(val_data4)\n",
    "\n",
    "dval4 = xgb.DMatrix(\n",
    "    data=val_data4.drop(columns=[\"iit\"]),\n",
    "    label=val_data4[\"iit\"]\n",
    ")\n",
    "# Test Near Data (July 4044)\n",
    "testnear_data4 = iit_data.copy()\n",
    "testnear_data4 = testnear_data4.drop(columns=[\"SiteCode\"])\n",
    "testnear_data4 = testnear_data4[(testnear_data4[\"NAD\"] >= \"2024-04-01\") & (testnear_data4[\"NAD\"] <= \"2024-04-30\")]\n",
    "testnear_data4 = testnear_data4.drop(columns=[\"key\", \"NAD\"])\n",
    "testnear_data4, _, _ = impute_data(\n",
    "    testnear_data4,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "testnear_data4 = encode_xgboost(testnear_data4)\n",
    "\n",
    "dtestnear4 = xgb.DMatrix(\n",
    "    data=testnear_data4.drop(columns=[\"iit\"]),\n",
    "    label=testnear_data4[\"iit\"]\n",
    ")\n",
    "\n",
    "# Test Data (Julyâ€“Sept 4044)\n",
    "test_data4 = iit_data.copy()\n",
    "test_data4 = test_data4.drop(columns=[\"SiteCode\"])\n",
    "test_data4 = test_data4[(test_data4[\"NAD\"] >= \"2024-04-01\") & (test_data4[\"NAD\"] <= \"2024-06-30\")]\n",
    "test_data4 = test_data4.drop(columns=[\"key\", \"NAD\"])\n",
    "test_data4, _, _ = impute_data(\n",
    "    test_data4,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "test_data4 = encode_xgboost(test_data4)\n",
    "\n",
    "dtest4 = xgb.DMatrix(\n",
    "    data=test_data4.drop(columns=[\"iit\"]),\n",
    "    label=test_data4[\"iit\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d6635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fold 5\n",
    "# Prepare Train Data (Janâ€“May 5055)\n",
    "train_data5 = iit_data.copy()\n",
    "train_data5 = train_data5.drop(columns=[\"SiteCode\"])\n",
    "train_data5 = train_data5[(train_data5[\"NAD\"] >= \"2024-01-01\") & (train_data5[\"NAD\"] <= \"2024-05-31\")]\n",
    "train_data5 = train_data5.drop(columns=[\"key\", \"NAD\"])\n",
    "\n",
    "# Impute train data and get imputation values\n",
    "train_data5, cat_impute_values, num_impute_values = impute_data(\n",
    "    train_data5,\n",
    "    fit=True,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "\n",
    "train_data5 = encode_xgboost(train_data5)\n",
    "\n",
    "dtrain5 = xgb.DMatrix(\n",
    "    data=train_data5.drop(columns=[\"iit\"]),\n",
    "    label=train_data5[\"iit\"]\n",
    ")\n",
    "\n",
    "# Validation Data (June 5055)\n",
    "val_data5 = iit_data.copy()\n",
    "val_data5 = val_data5.drop(columns=[\"SiteCode\"])\n",
    "val_data5 = val_data5[(val_data5[\"NAD\"] >= \"2024-06-01\") & (val_data5[\"NAD\"] <= \"2024-06-30\")]\n",
    "val_data5 = val_data5.drop(columns=[\"key\", \"NAD\"])\n",
    "val_data5, _, _ = impute_data(\n",
    "    val_data5,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "val_data5 = encode_xgboost(val_data5)\n",
    "\n",
    "dval5 = xgb.DMatrix(\n",
    "    data=val_data5.drop(columns=[\"iit\"]),\n",
    "    label=val_data5[\"iit\"]\n",
    ")\n",
    "# Test Near Data (July 5055)\n",
    "testnear_data5 = iit_data.copy()\n",
    "testnear_data5 = testnear_data5.drop(columns=[\"SiteCode\"])\n",
    "testnear_data5 = testnear_data5[(testnear_data5[\"NAD\"] >= \"2024-07-01\") & (testnear_data5[\"NAD\"] <= \"2024-07-31\")]\n",
    "testnear_data5 = testnear_data5.drop(columns=[\"key\", \"NAD\"])\n",
    "testnear_data5, _, _ = impute_data(\n",
    "    testnear_data5,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "testnear_data5 = encode_xgboost(testnear_data5)\n",
    "\n",
    "dtestnear5 = xgb.DMatrix(\n",
    "    data=testnear_data5.drop(columns=[\"iit\"]),\n",
    "    label=testnear_data5[\"iit\"]\n",
    ")\n",
    "\n",
    "# Test Data (Julyâ€“Sept 5055)\n",
    "test_data5 = iit_data.copy()\n",
    "test_data5 = test_data5.drop(columns=[\"SiteCode\"])\n",
    "test_data5 = test_data5[(test_data5[\"NAD\"] >= \"2024-07-01\") & (test_data5[\"NAD\"] <= \"2024-09-30\")]\n",
    "test_data5 = test_data5.drop(columns=[\"key\", \"NAD\"])\n",
    "test_data5, _, _ = impute_data(\n",
    "    test_data5,\n",
    "    cat_impute_values=cat_impute_values,\n",
    "    num_impute_values=num_impute_values,\n",
    "    categorical_cols=categorical_cols,\n",
    "    numeric_cols=numeric_cols\n",
    ")\n",
    "test_data5 = encode_xgboost(test_data5)\n",
    "\n",
    "dtest5 = xgb.DMatrix(\n",
    "    data=test_data5.drop(columns=[\"iit\"]),\n",
    "    label=test_data5[\"iit\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_list = {\n",
    "    \"fold1\": [dtrain1, dval1, dtestnear1, dtest1, testnear_data1, test_data1],\n",
    "    \"fold2\": [dtrain2, dval2, dtestnear2, dtest2, testnear_data2, test_data2],\n",
    "    \"fold3\": [dtrain3, dval3, dtestnear3, dtest3, testnear_data3, test_data3],\n",
    "    \"fold4\": [dtrain4, dval4, dtestnear4, dtest4, testnear_data4, test_data4],\n",
    "    \"fold5\": [dtrain5, dval5, dtestnear5, dtest5, testnear_data5, test_data5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c7862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# Step 1: Create the grid\n",
    "params_grid = list(product(\n",
    "    [0.05],                  # eta\n",
    "    [6, 8],               # max_depth\n",
    "    [0.5, 0.8],           # subsample\n",
    "    [0.5, 0.8],           # colsample_bytree\n",
    "    [1,10],                     # lambda\n",
    "    [20,50]                  # scale_pos_weight\n",
    "))\n",
    "\n",
    "# Build DataFrame\n",
    "grid_sparse = pd.DataFrame(params_grid, columns=[\n",
    "    \"eta\", \"max_depth\", \"subsample\", \"col_sample\", \"lambda_\", \"scale_pos_weight\"\n",
    "])\n",
    "# Add empty columns for PR AUCs\n",
    "for k in range(1, 6):\n",
    "    grid_sparse[f\"val_pr_auc_near_{k}\"] = np.nan\n",
    "    grid_sparse[f\"val_pr_auc_{k}\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e45c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for each grid row and each fold\n",
    "for i in tqdm(range(len(grid_sparse)), desc=\"Grid Search\"):\n",
    "\n",
    "    row = grid_sparse.iloc[i]\n",
    "\n",
    "    for k in tqdm(range(1, 6), desc=f\"Fold {i+1}\", leave=False):\n",
    "\n",
    "        dtrain, dval, dtestnear, dtest, test_near, test_data = fold_list[f\"fold{k}\"]\n",
    "\n",
    "        params = {\n",
    "            \"eta\": row[\"eta\"],\n",
    "            \"max_depth\": int(row[\"max_depth\"]),\n",
    "            \"subsample\": row[\"subsample\"],\n",
    "            \"colsample_bytree\": row[\"col_sample\"],\n",
    "            \"lambda\": row[\"lambda_\"],\n",
    "            \"scale_pos_weight\": row[\"scale_pos_weight\"],\n",
    "            \"eval_metric\": \"aucpr\",\n",
    "            \"objective\": \"binary:logistic\"\n",
    "        }\n",
    "\n",
    "        xgb_model = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=3000,\n",
    "            evals=[(dtrain, \"train\"), (dval, \"val\")],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "\n",
    "        # Predict on dtestnear\n",
    "        testnear_preds = xgb_model.predict(dtestnear)\n",
    "        ap_near = average_precision_score(test_near[\"iit\"], testnear_preds)\n",
    "        grid_sparse.at[grid_sparse.index[i], f\"val_pr_auc_near_{k}\"] = ap_near\n",
    "\n",
    "        # Predict on dtest\n",
    "        test_preds = xgb_model.predict(dtest)\n",
    "        ap = average_precision_score(test_data[\"iit\"], test_preds)\n",
    "        grid_sparse.at[grid_sparse.index[i], f\"val_pr_auc_{k}\"] = ap\n",
    "\n",
    "    print(grid_sparse.iloc[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "s3 = boto3.client('s3')  # assumes you've run aws configure or have IAM role\n",
    "# Create a CSV in memory\n",
    "csv_buffer = StringIO()\n",
    "grid_sparse.to_csv(csv_buffer, index=False)\n",
    "s3.put_object(\n",
    "    Bucket='kehmisjan2025',\n",
    "    Key='xgbgrid_sparse_050525.csv',\n",
    "    Body=csv_buffer.getvalue()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
